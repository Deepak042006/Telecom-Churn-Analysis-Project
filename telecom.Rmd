---
title: "Telecom churn Analysis"
author: "Deepak.s"
date: "2024-12-19"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

#1)Data understanding

```{r}
suppressMessages(library(tidyverse))
suppressMessages(library(caret))
suppressMessages(library(reshape2))
suppressMessages(library(broom))
suppressMessages(library(randomForest))
suppressMessages(library(performanceEstimation))
suppressMessages(library(regclass))
suppressMessages(library(GGally))
suppressMessages(library(pROC))
suppressMessages(library(plotROC))
suppressMessages(library(cowplot))
suppressMessages(library(grid))
suppressMessages(library(gridExtra))
suppressMessages(library(formattable))
suppressMessages(library(scales))
suppressMessages(library(ggplot2))
library(kernlab)
theme_set(theme_minimal())
options(warn=-1)
```


```{r}
telecom <- read_csv("C:/Users/Deepak/Documents/Project details/WA_Fn-UseC_-Telco-Customer-Churn.csv")
view(telecom)
```
```{r}
str(telecom)
```
```{r}
summary(telecom)
```
##The Data description

customerID : Customer ID
gender : Whether the customer is a male or a female
SeniorCitizen : Whether the customer is a senior citizen or not (1, 0)
Partner : Whether the customer has a partner or not (Yes, No)
Dependents : Whether the customer has dependents or not (Yes, No)
tenure : Number of months the customer has stayed with the company
PhoneService : Whether the customer has a phone service or not (Yes, No)
MultipleLines : Whether the customer has multiple lines or not (Yes, No, No phone service)
InternetService : Customerâ€™s internet service provider (DSL, Fiber optic, No)
OnlineSecurity : Whether the customer has online security or not (Yes, No, No internet service)
OnlineBackup : Whether the customer has online backup or not (Yes, No, No internet service)
DeviceProtection : Whether the customer has device protection or not (Yes, No, No internet service)
TechSupport : Whether the customer has tech support or not (Yes, No, No internet service)
StreamingTV : Whether the customer has streaming TV or not (Yes, No, No internet service)
StreamingMovies : Whether the customer has streaming movies or not (Yes, No, No internet service)
Contract : The contract term of the customer (Month-to-month, One year, Two year)
PaperlessBilling : Whether the customer has paperless billing or not (Yes, No)
PaymentMethod : The customerâ€™s payment method (Electronic check, Mailed check, Bank transfer (automatic), Credit card (automatic))
MonthlyCharges : The amount charged to the customer monthly
TotalCharges : The total amount charged to the customer
Churn : Whether the customer churned or not (Yes or No)



#Data Understanding

```{r}
summary(telecom)
```

#2)Data Pre processing

```{r}
# --- Step 1: Handle Missing Values ---
# Check for missing values
colSums(is.na(telecom))

```
```{r}
#convert to numeric and remove na values in dataset
telecom$TotalCharges <- as.numeric(as.character(telecom$TotalCharges)) 
telecom <- telecom %>% na.omit()
```

```{r}
#again check is there NA value in data set
colSums(is.na(telecom))

```

```{r}
# --- Step 2: Explore Categorical Variables ---
# Identify categorical variables
categorical_vars <- telecom %>%
  select_if(is.character)
# Remove the customerID column from categorical variables for shot the output it is also a categorical variable
categorical_vars <- categorical_vars %>%
  select(-customerID)
# Add SeniorCitizen column as a categorical variable
categorical_vars <- telecom %>%
  mutate(SeniorCitizen = as.factor(SeniorCitizen)) %>% # Convert SeniorCitizen to a factor
  select(c(colnames(categorical_vars), "SeniorCitizen")) # Include SeniorCitizen with other categorical variables
# Display unique categories for each categorical variable
lapply(categorical_vars, function(column) {
  unique(column)
})
```
##these are the categorical variables in telecom data set

gender
Partner
Dependents
PhoneService
MultipleLines
InternetService
OnlineSecurity
OnlineBackup
DeviceProtection
TechSupport
StreamingTV
StreamingMovies
Contract
PaperlessBilling
PaymentMethod
Churn
SeniorCitizen

```{r}

# Remove unnessary column customer id and Convert character variables to factors
telecom <- telecom %>%
  select(-customerID)%>% 
  mutate_at(7, ~as.factor(case_when(. == "No phone service" ~ "No",
                                    . == "No" ~ "No", TRUE ~ "Yes"))) %>% 
  mutate_at(8, ~as.factor(case_when(. == "Fiber optic" ~ "FiberOptic",
                                    . == "DSL" ~ "DSL", TRUE ~ "No"))) %>% 
  mutate_at(c(9:14), ~as.factor(case_when(. == "No internet service" ~ "No", 
                                          . == "No" ~ "No", TRUE ~ "Yes"))) %>%
  mutate_at(17, ~as.factor(case_when(. == "Bank transfer (automatic)" ~ "BankTransferAuto", 
                                     . == "Credit card (automatic)" ~ "CreditCardAuto", 
                                     . == "Electronic check" ~ "ECheck", TRUE ~ "MailedCheck")))
# Convert character variables to factors
telecom <- telecom %>%
  mutate(across(where(is.character), as.factor))
```

```{r}
telecom %>% 
  group_by(gender) %>% 
  rename("Gender" = gender) %>% 
  summarise("Number of Observations" = n(),
            "Average Tenure, in months" = round(mean(tenure), 0),
            "Monthly Charges" = round(mean(MonthlyCharges), 2))
```
Based on the overall gender composition of our sample, there is an approximately equal proportion of men and women in the data set. Their average bill is around $65/month, and the tenure of both groups is a little over 2 and a half years, with men staying slightly longer than women on average.



#3) Exploratory Data Analysis (EDA) Using R

##3.1 | What is a customer's average tenure with Telco and what are their average charges?

```{r}
t2 <- telecom %>% 
  mutate(Churn2 = as.factor(ifelse(Churn == "Yes", "Former Customers", "Current Customers"))) 

g1 <- ggplot(t2, aes(x = fct_rev(Churn2), y = tenure, fill = fct_rev(Churn2))) +
  geom_bar(stat = "summary", fun = "mean", alpha = 0.6, color = "grey20", show.legend = F) +
  stat_summary(aes(label = paste(round(..y.., 0), "months")), fun = mean, 
               geom = "text", size = 3.5, vjust = -0.5) +
  labs(title = "Average Customer Tenure \n", x = "", y = "Customer Tenure\n") +
  theme(plot.title = element_text(hjust = 0.5))

g2 <- ggplot(t2, aes(x = fct_rev(Churn2), y = MonthlyCharges, fill = fct_rev(Churn2))) +
  geom_bar(stat = "summary", fun = "mean", alpha = 0.6, color = "grey20", show.legend = F) +
  stat_summary(aes(label = dollar(..y..)), fun = mean, 
               geom = "text", size = 3.5, vjust = -0.5) +
  scale_y_continuous(labels = dollar_format()) +
  labs(title = "Average Monthly Charges \n", x = "", y = "Monthly Charges \n") +
  theme(plot.title = element_text(hjust = 0.5))

g3 <- ggplot(t2, aes(x = Contract, y = MonthlyCharges, fill = fct_rev(Churn2))) +
  geom_bar(position = "dodge", stat = "summary", fun = "mean", alpha = 0.6, color = "grey20") +
  stat_summary(aes(label = dollar(..y..)), fun = mean, 
               geom = "text", size = 3.5, vjust = -0.5,
               position = position_dodge(width = 0.9)) +
  coord_cartesian(ylim = c(0, 95)) +
  scale_y_continuous(labels = dollar_format()) +
  labs(title = "\nAverage Monthly Charges by Contract Type", x = "\n Contract Type", 
       y = "Monthly Charges \n", fill = "") +
  theme(plot.title = element_text(hjust = 0.5), legend.position = "top", legend.justification = "left")

options(repr.plot.width=10, repr.plot.height=14)
grid.arrange(g1, g2, ncol = 2, nrow = 1, layout_matrix = rbind(c(1,2)))

```

```{r}
grid.arrange( g3, ncol = 2, nrow = 1, layout_matrix = rbind(c(3,3)))
```
The graphs above show the average tenure of Telco's current and former customers and their monthly charges. Telco's current customers have been with the company for just over 3 years, while customers who left kept their services for about 18 months. Additionally, former customers had higher monthly charges on average by about $13. This holds true across each contract type.


##3.2 | What type of account services do customers have?
```{r}
g1 <- ggplot(t2, aes(x = Contract, group = fct_rev(Churn2))) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = "count",
           alpha = 0.6, color = "grey20", show.legend = F) +
  geom_text(aes(label = percent(..prop..), y = ..prop.. ), 
            size = 4, stat = "count", vjust = -0.5) +
  facet_grid(~fct_rev(Churn2)) +
  scale_y_continuous(labels = percent_format()) +
  coord_cartesian(ylim = c(0, .95)) +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "Customer Churn by Contract Type\n", x = "\n Contract Type", y = "") +
  theme(plot.title = element_text(hjust = 0.5))

g2 <- ggplot(t2, aes(x = InternetService, group = fct_rev(Churn2))) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = "count",
           alpha = 0.6, color = "grey20", show.legend = F) +
  geom_text(aes(label = percent(..prop..), y = ..prop.. ), 
            size = 4, stat = "count", vjust = -0.5) +
  facet_grid(~fct_rev(Churn2)) +
  scale_y_continuous(labels = percent_format()) +
  coord_cartesian(ylim = c(0, .9)) +
  scale_fill_brewer(palette = "Paired") +
  labs(title = "\n Customer Churn by Internet Service \n", x = "\n Internet Service", y = "") +
  theme(plot.title = element_text(hjust = 0.5))

grid.arrange(g1, ncol = 1)
```
```{r}
grid.arrange(g2, ncol = 1)
```
Nearly 89% of former customers were on month-to-month contracts, with a much smaller proportion in one or two-year contracts. Of customers who left, a little over 69% had Fiber Optic internet. This could be an indicator of potential dissatisfaction with the service and should be further reviewed by the company since currently over a third of their customers have this type of internet.


##3.3 | Customer Attrition Demographics

```{r}
g1 <- ggplot(t2, aes(x = fct_rev(ifelse(SeniorCitizen==1, "Yes", "No")), group = Churn2)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = "count",
           alpha = 0.6, color = "grey20", show.legend = F) +
  geom_text(aes(label = percent(..prop.., accuracy = 0.1), y = ..prop..), 
            size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
  facet_grid(~fct_rev(Churn2)) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  coord_cartesian(ylim = c(0, .9)) +
  labs(x = "\n Senior Citizen", y = "")

g2 <- ggplot(t2, aes(x = gender, group = Churn2)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat = "count",
           alpha = 0.6, color = "grey20", show.legend = F) +
  geom_text(aes(label = percent(..prop.., accuracy = 0.1), y = ..prop..), 
            size = 4, stat = "count", position = position_stack(vjust = 0.5)) +
  facet_grid(~fct_rev(Churn2)) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  coord_cartesian(ylim = c(0, .6)) +
  labs(x = "\n Gender", y = "")

options(repr.plot.width=18, repr.plot.height=7)
grid.arrange(g1, g2, nrow = 1, top = textGrob("Customer Attrition Demographics \n",
                                              gp = gpar(fontsize = 14)))

```

Based on the demographic attributes of Telco's customers, about a quarter of those who left were senior citizens, and just under 13% of their current customers are 65 years or older. The distribution of gender is proportional in both current and former customers, with an approximately equal number of men and women leaving within the last month.



##3.4 | Distributions and Correlations


```{r}
options(repr.plot.width=12, repr.plot.height=10)
telecom %>% 
  select(tenure, MonthlyCharges, TotalCharges, Churn) %>%
  ggpairs(aes(color = fct_rev(Churn)), title = "Customer Account Distributions and Correlations \n",
          columnLabels = c("Tenure", "Monthly Charges", "Total Charges", "Churn"),
          upper = list(combo = wrap("box_no_facet", alpha = 0.7)),
          diag = list(continuous = wrap("densityDiag", alpha = 0.6), 
                      discrete = wrap("barDiag", alpha = 0.7, color = "grey30")),
          lower = list(combo = wrap("box_no_facet", alpha = 0.7), continuous = wrap("smooth", alpha = 0.15)))
```
##3.5| SMOTE for Churn Balance

Our target variable, Churn, is quite imbalanced with a little over 26% (1,869 customers) leaving the company within the past month. Since class imbalance can negatively affect the precision and recall accuracy of statistical models, I will use a synthetic minority over-sampling technique known as smote to create a more evenly distributed training set.

The smote algorithm artificially generates new instances of the minority class using the nearest neighbors of these cases and under-samples the majority class to create a more balanced data set. After applying smote, our training set now consists of an equal proportion of current and former customers

```{r}
telecom <- telecom %>% 
  mutate_at(15, ~as.factor(case_when(. == "One year" ~ "OneYear", 
                                     . == "Two year" ~ "TwoYear", 
                                     TRUE ~ "Month-to-month"))) 

set.seed(1)
ind <- createDataPartition(telecom$Churn, p = 0.7, list = F)
telecom.train <- telecom[ind,]
telecom.test <- telecom[-ind,]
train.resamp <- smote(Churn ~ ., data = data.frame(telecom.train), perc.over = 1, perc.under = 2)

g1 <- ggplot(t2, aes(x = fct_rev(Churn2), fill = fct_rev(Churn2))) +
  geom_bar(alpha = 0.6, color = "grey30", show.legend = F) + 
  geom_text(stat = "count", size = 3.5, 
            aes(label = paste("n = ", formatC(..count.., big.mark = ","))), vjust = -0.5) +
  scale_y_continuous(labels = comma_format()) +
  labs(subtitle = "Before Resampling\n", x = "", y = "Number of Customers\n")

g2 <- ggplot(train.resamp, aes(x = fct_rev(ifelse(Churn == "Yes", "Former Customers", "Current Customers")), 
                               fill = fct_rev(Churn))) +
  geom_bar(alpha = 0.6, color = "grey30", show.legend = F) + 
  geom_text(stat = "count", size = 3.5, 
            aes(label = paste("n = ", formatC(..count.., big.mark = ","))), vjust = -0.5) +
  scale_y_continuous(labels = comma_format()) +
  labs(subtitle = "After Resampling\n", x = "", y = "")

options(repr.plot.width=9, repr.plot.height=7)
grid.arrange(g1, g2, nrow = 1, top = textGrob("Distribution of Customer Churn\n",
                                              gp = gpar(fontsize = 14)))

```
##3.6 | Feature Selection

To identify which features should be included in the models, I will use a two-step process. First, I will check the chi-squared tests of independence between the categorical features and include only variables that have a statistically significant association to our response, Churn. Then, I will use the random forest algorithm to identify the most important predictors of customer churn.


###3.6.1 | Chi-Squared Tests

The Chi-Squared Test of Independence evaluates the association between two categorical variables. The null hypothesis for this test is that there is no relationship between our response variable and the categorical feature, and the alternative hypothesis is that that there is a relationship. Looking at the results of the tests, Gender and PhoneService have very small chi-squared statistics and p-values that are greater than the significance threshold, a, of 0.05, indicating they are independent of our target variable. The rest of the categorical features do have a statistically significant association to customer churn.


```{r}

# Perform Chi-Squared tests, excluding the target variable from the predictors
chi <- lapply(names(categorical_vars)[-17], function(col_name) {
  result <- chisq.test(categorical_vars[, 17], categorical_vars[[col_name]])
  result$variable <- col_name # Add the variable name
  result
})

# Convert Chi-Squared test results into a tidy data frame with variable names
chi_results <- do.call(rbind, lapply(chi, function(res) {
  tidy_res <- broom::tidy(res)
  tidy_res$variable <- res$variable # Add the variable name to the tidy results
  tidy_res
})) %>%
  arrange(p.value) %>% 
  mutate(across(c(statistic, p.value), ~ round(., 3)))

# View the results
chi_results
```


#4 | Predicting Customer Churn


##4.1 | Methodology
To predict which customers are most likely to churn, several different types of classification models will be evaluated, including logistic regression, support vector machines, and random forests, KNN. Since the numeric predictors, MonthlyCharges and Tenure, have skewed distributions and varying scales, I will apply a preprocessing technique that normalizes the features to have a mean of 0 and a standard deviation of 1.
###10-fold cross-validation, why i select this method

1. 70%-30% Train-Test Split
What it Does: Splits data into:
       70% Training Set: To train the model.
       30% Test Set: To evaluate performance on unseen data.
Purpose: Quick and easy way to check model performance.
Advantages:
       Simple and fast.
       Suitable for large datasets.
Disadvantages:
       Results vary with different random splits.
       Not ideal for small datasets as it doesn't use all data for training.
       
2. 10-Fold Cross-Validation

What it Does:
       Splits data into 10 parts (folds).
       In each iteration:
       
         9 folds = Training set.
         1 fold = Test set.
Repeats 10 times, using each fold as the test set once.
        Final result = Average of all 10 iterations.
        Purpose: More reliable evaluation by minimizing randomness.
Advantages:
        Uses all data for training and testing.
        Provides more stable and accurate results.
Disadvantages:
        Slower due to multiple training/testing cycles.
        Slightly more complex than a single train-test split.
When to Use Each?

70%-30% Split:
        Best for large datasets.
        Ideal for quick evaluations or limited computational power.
10-Fold Cross-Validation:
        Best for small datasets where every point matters.
        Preferred for thorough and reliable evaluations.


To fit the models, 10-fold cross-validation will be used and the model will be tested on the out of sample dataset. This set was held out of resampling and is more representative of the true class distribution.

##4.2 | Logistic Regression
Logistic regression is a parametric classification technique that estimates the probability of an event occurring, for instance, whether or not a customer will leave the company. One of the advantages of the logistic model is the interpretability of the model parameters. Based on the size of the coefficients and the significance of the predictors, the model is able to quantify the relationships between our response and the input features.


```{r}
set.seed(21)
ctrl <- trainControl(method = "cv", number = 10, classProbs = TRUE, 
                     summaryFunction = twoClassSummary)

glm.fit <- train(Churn ~ tenure + MonthlyCharges + InternetService + PaymentMethod + 
                 Contract + OnlineSecurity + TechSupport + PaperlessBilling, 
                 data = train.resamp, method = "glm", metric = "ROC",
                 preProcess = c("center", "scale"), trControl = ctrl)
glm.preds <- glm.fit %>% predict(telecom.test)
glm.cm <- data.frame(Logistic=confusionMatrix(glm.preds, telecom.test$Churn, 
                                              positive = "Yes", mode = "everything")$byClass)
confusionMatrix(glm.preds, telecom.test$Churn, positive = "Yes", mode = "everything")

```

Our logistic regression model has an overall accuracy of 76.1% and a precision of 53.3% on the test set. This means that when the model predicts a customer will leave, it is correct around 54% of the time. The recall of our model is 80.7%, which means that it correctly identified about 81% of all customers who left.

##4.3 | Support Vector Machine
Support vector machines (SVMs) are a commonly used statistical learning model. It is nonparametric, which means that it does not make any assumptions about the data like logistic regression does. SVMs involve finding a hyperplane that separates the data as well as possible and maximizes the distance between the classes of our response variable.



```{r}

svm.fit <- train(Churn ~ tenure + MonthlyCharges + InternetService + PaymentMethod + 
                 Contract + OnlineSecurity + TechSupport + PaperlessBilling, 
                 data = train.resamp, method = "svmLinear", metric = "ROC",
                 preProcess = c("center","scale"), trControl = ctrl)
svm.preds <- svm.fit %>% predict(telecom.test)
svm.cm <- data.frame(SVM=confusionMatrix(svm.preds, telecom.test$Churn, 
                                         positive = "Yes", mode = "everything")$byClass)
confusionMatrix(svm.preds, telecom.test$Churn, positive = "Yes", mode = "everything")
```
The accuracy of the linear support vector machine is about 70% and the precision is 46%, which is not an improvement from the previous models. The recall did increase to 85%, which is the highest so far.

##4.4 | Random Forest
Random forest is a commonly used ensemble technique in machine learning. The model is built using a combination of many decision trees, where each takes a random sample of the data with replacement and selects a random subset of predictors, resulting in a relatively uncorrelated set of decision trees. Each tree then makes a prediction and the class with the most votes becomes the model's final prediction.

```{r}
rf.fit <- train(Churn ~ tenure + MonthlyCharges + InternetService + PaymentMethod + 
                Contract + OnlineSecurity + TechSupport + PaperlessBilling,
                data = train.resamp, method = "rf", metric = "ROC",
                preProcess = c("center","scale"), trControl = ctrl)
rf.preds <- rf.fit %>% 
  predict(telecom.test)
rf.cm <- data.frame(rf=confusionMatrix(rf.preds, telecom.test$Churn, 
                                       positive = "Yes", mode = "everything")$byClass)
confusionMatrix(rf.preds, telecom.test$Churn, positive = "Yes", mode = "everything")
```
The random forest classifier has an accuracy of 74% and a precision of 50%, higher than the SVM but just below our logistic model. The recall of the model is about 75%, the lowest overall.


##4.5 | K-Nearest Neighbors
K-Nearest Neighbors (KNN) is a simple, non-parametric machine learning algorithm used for classification and regression tasks. It predicts the outcome based on the majority class or average value of its k nearest neighbors in the feature space. KNN is sensitive to the choice of k (the number of neighbors) and the scaling of the input features.
```{r}
#Train KNN model
set.seed(123)  # Set seed for reproducibility
knn.fit <- train(
  Churn ~ tenure + MonthlyCharges + InternetService + PaymentMethod +
          Contract + OnlineSecurity + TechSupport + PaperlessBilling,
  data = train.resamp,
  method = "knn",
  tuneLength = 10,      # Automatically tune k from a grid of 10 values
  metric = "ROC",       # Optimize for ROC metric
  trControl = ctrl
)
knn.preds <- predict(knn.fit, telecom.test)
knn.cm <- data.frame(knn=confusionMatrix(knn.preds, telecom.test$Churn, 
                                       positive = "Yes", mode = "everything")$byClass)
confusionMatrix(knn.preds, telecom.test$Churn, positive = "Yes", mode = "everything")

```
The K-Nearest Neighbors (KNN) classifier achieves an accuracy of 72.5%, which is slightly below the Random Forest and logistic regression models. The precision of the model is 48%, indicating that nearly half of the positive predictions are correct. The recall is 77%, which is lower than the the Random Forest model and logistic regression model.

#5 | Model Evaluation and ROC Curves


##5.1 | Model Performance on the Test Set

```{r}
res.cm <- data.frame(glm.cm, svm.cm, rf.cm, knn.cm) %>% 
  rename("Random Forest" = rf) 
res <- data.frame(t(res.cm))
rownames(res) <- colnames(res.cm)
colnames(res) <- rownames(res.cm)
res[,c(7,5,6,2,11)] %>% 
  arrange(desc(F1)) %>% 
  mutate_all(percent_format(accuracy = 0.1))
```
Out of the four models, logistic regression produces the highest F1 score, which represents the balance between precision and recall, as well as the highest specificity, which measures how well the model identifies negative cases correctly.


##5.2 | ROC Curves

As a final step in model selection, I will plot the ROC curves of each model with their corresponding Area Under the Curve (AUC). The Area Under the Curve measures the model's performance across all possible classification thresholds. A higher AUC indicates the model is better able to distinguish between the classes.


```{r}
Logistic <- predict(glm.fit, telecom.test, type = "prob")[,2]
SVM <- predict(svm.fit, telecom.test, type = "prob")[,2]
RandomForest <- predict(rf.fit, telecom.test, type = "prob")[,2]
KNN <- predict(knn.fit, telecom.test, type = "prob")[,2]

roc.data <- cbind(telecom.test[,20], Logistic, SVM, RandomForest, KNN)

```



```{r}
# Ensure "Churn" is the target variable in the test set
roc.data <- data.frame(Churn = telecom.test$Churn, Logistic, SVM, RandomForest, KNN)

# Reshape the data for ROC plotting using tidyr
library(tidyr)
library(dplyr)

# Reshape the data into a long format
roc.long <- roc.data %>%
  pivot_longer(cols = c(Logistic, SVM, RandomForest, KNN), 
               names_to = "Model", 
               values_to = "Prediction")

# Convert Churn to binary values (1 for "Yes", 0 for "No")
roc.long$Churn <- ifelse(roc.long$Churn == "Yes", 1, 0)

# Check the structure of the reshaped data
str(roc.long)

# Now generate the ROC plot
rocplot <- ggplot(roc.long, aes(d = Churn, m = Prediction, color = Model)) +
  geom_roc(n.cuts = 0) +
  style_roc(xlab = "\nFalse Positive Rate (1 - Specificity)", 
            ylab = "True Positive Rate (Sensitivity)\n") +
  labs(title = "ROC Curve Comparison on the Test Set", color = "Model") +
  theme(plot.title = element_text(hjust = 0.5))

# Add AUC values and abline
rocplot +
  geom_abline(size = 0.5, color = "grey30") +
  annotate("text", x = 0.77, y = 0.35, label = paste("AUC of Logistic =", round(calc_auc(rocplot)$AUC[2], 3))) +
  annotate("text", x = 0.75, y = 0.28, label = paste("AUC of SVM =", round(calc_auc(rocplot)$AUC[4], 3))) +
  annotate("text", x = 0.75, y = 0.21, label = paste("AUC of Random Forest =", round(calc_auc(rocplot)$AUC[3], 3))) +
  annotate("text", x = 0.74, y = 0.14, label = paste("AUC of KNN =", round(calc_auc(rocplot)$AUC[1], 3))) +
  scale_color_discrete(breaks = c("Logistic", "SVM", "RandomForest", "KNN"))

```
Out of the four classifiers, the logistic model has the highest Area Under the Curve of 0.854 on the test set. This represents the probability that our model will rate or rank a randomly chosen observation from the positive class, Churn = Yes, as more likely to be from that class than a randomly chosen nonpositive observation, Churn = No (Hanley & McNeil, 1982).
```{r}
library(ggplot2)
library(caret)

# Assuming you have already trained the models and have predictions for each model:
# glm.fit (Logistic Regression), svm.fit (SVM), rf.fit (Random Forest), knn.fit (KNN)

# Make predictions
glm.preds <- predict(glm.fit, telecom.test)
svm.preds <- predict(svm.fit, telecom.test)
rf.preds <- predict(rf.fit, telecom.test)
knn.preds <- predict(knn.fit, telecom.test)

# Compute accuracy for each model
glm.acc <- confusionMatrix(glm.preds, telecom.test$Churn)$overall['Accuracy']
svm.acc <- confusionMatrix(svm.preds, telecom.test$Churn)$overall['Accuracy']
rf.acc <- confusionMatrix(rf.preds, telecom.test$Churn)$overall['Accuracy']
knn.acc <- confusionMatrix(knn.preds, telecom.test$Churn)$overall['Accuracy']

# Create a data frame for plotting
acc_data <- data.frame(
  Model = c("Logistic Regression", "SVM", "Random Forest", "KNN"),
  Accuracy = c(glm.acc, svm.acc, rf.acc, knn.acc)
)

# Plot the accuracies
ggplot(acc_data, aes(x = Model, y = Accuracy, fill = Model)) +
  geom_bar(stat = "identity", show.legend = FALSE) +
  ylim(0, 1) +
  labs(title = "Model Accuracy Comparison", y = "Accuracy", x = "Model") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
```{r}
library(ggplot2)
library(caret)

# Assuming the models are already trained (glm.fit, svm.fit, rf.fit, knn.fit)

# Make predictions for each model
glm.preds <- predict(glm.fit, telecom.test)
svm.preds <- predict(svm.fit, telecom.test)
rf.preds <- predict(rf.fit, telecom.test)
knn.preds <- predict(knn.fit, telecom.test)

# Calculate Balanced Accuracy and F1 Score for each model
glm.cm <- confusionMatrix(glm.preds, telecom.test$Churn, positive = "Yes")
svm.cm <- confusionMatrix(svm.preds, telecom.test$Churn, positive = "Yes")
rf.cm <- confusionMatrix(rf.preds, telecom.test$Churn, positive = "Yes")
knn.cm <- confusionMatrix(knn.preds, telecom.test$Churn, positive = "Yes")

# Extract Balanced Accuracy and F1 Score from confusion matrices
glm.balanced_acc <- glm.cm$byClass['Balanced Accuracy']
svm.balanced_acc <- svm.cm$byClass['Balanced Accuracy']
rf.balanced_acc <- rf.cm$byClass['Balanced Accuracy']
knn.balanced_acc <- knn.cm$byClass['Balanced Accuracy']

glm.f1 <- glm.cm$byClass['F1']
svm.f1 <- svm.cm$byClass['F1']
rf.f1 <- rf.cm$byClass['F1']
knn.f1 <- knn.cm$byClass['F1']

# Create a data frame for plotting
acc_f1_data <- data.frame(
  Model = c("Logistic Regression", "SVM", "Random Forest", "KNN"),
  Balanced_Accuracy = c(glm.balanced_acc, svm.balanced_acc, rf.balanced_acc, knn.balanced_acc),
  F1_Score = c(glm.f1, svm.f1, rf.f1, knn.f1)
)

# Reshape the data to long format for plotting
acc_f1_long <- reshape(acc_f1_data, 
                       varying = c("Balanced_Accuracy", "F1_Score"), 
                       v.names = "Score", 
                       timevar = "Metric", 
                       times = c("Balanced Accuracy", "F1 Score"), 
                       direction = "long")

# Plot the Balanced Accuracy vs. F1 Score
ggplot(acc_f1_long, aes(x = Model, y = Score, fill = Metric)) +
  geom_bar(stat = "identity", position = "dodge") +
  labs(title = "Balanced Accuracy vs F1 Score for Different Models", 
       y = "Score", 
       x = "Model") +
  scale_fill_manual(values = c("cyan", "purple")) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) 

```
##5.3 | Key Findings

Overall, the logistic regression model had the strongest performance on the test set. Based on the coefficients from the model, at least one category in all eight predictors has a significant association to customer attrition. A summary of the relationships of each, when all other variables are held constant, is listed in the table below.


```{r}
glm.fit <- train(Churn ~ tenure + MonthlyCharges + InternetService + PaymentMethod + 
                 Contract + OnlineSecurity + TechSupport + PaperlessBilling, 
                 data = telco, method = "glm", 
                 preProcess = c("center", "scale"), 
                 trControl = trainControl(method = "cv", number = 10))
summary(glm.fit$finalModel)
```
```{r}
OR <- coef(glm.fit$finalModel) %>% exp() %>% round(digits = 2) %>% as.data.frame() %>% slice(-c(1,6,8))
data.frame(Predictor = c("Tenure", "MonthlyCharges", "InternetServiceFiberOptic", 
                         "InternetServiceNo", "PaymentMethodECheck", "ContractOneYear", "ContractTwoYear",
                         "OnlineSecurityYes", "TechSupportYes", "PaperlessBillingYes"),
           OddsRatio = OR[,1]) %>% 
  arrange(desc(OddsRatio))
```
```{r}
OR <- coef(glm.fit$finalModel) %>% exp() %>% round(digits = 2) %>% as.data.frame() %>% slice(-c(1,6,8))
data.frame(Predictor = c("Tenure", "MonthlyCharges", "InternetServiceFiberOptic", 
                         "InternetServiceNo", "PaymentMethodECheck", "ContractOneYear", "ContractTwoYear",
                         "OnlineSecurityYes", "TechSupportYes", "PaperlessBillingYes"),
           OddsRatio = OR[,1],
           Interpretation = c("A one month increase in tenure decreases the risk of churning by about 53%.",
                              "For every $1 increase in monthly charges, we expect to see an increase in 
                              the odds of churning by a factor of 1.39 or by 39%.",
                              "Customers with fiber optic internet are 31% more likely to churn than those 
                              with DSL.", "Those without internet are 28% less likely to churn than 
                              customers with DSL internet.", "Customers who pay with electronic checks are 
                              more likely to churn by a factor of 1.19 or by 19% compared to customers who use 
                              automatic bank transfers.", "Customers on one-year contracts are 25% less likely 
                              to churn than customers on month-to-month contracts.", "Customers 
                              on two-year contracts are 44% less likely to churn compared to those on 
                              month-to-month contracts.", "Customers with online security are 19% less likely 
                              to churn than customers without online security.", "Customers with tech support 
                              are about 17% less likely to churn than customers without tech support.", 
                              "Customers with paperless billing are 21% more likely to churn than customers 
                              without paperless billing.")) %>% 
  arrange(desc(OddsRatio)) %>% view()

```
#6 | Conclusion

Â¶
In predicting customer attrition, logistic regression produced the highest Area Under the Curve, F1 score, and specificity. Some of the most important predictors of customer attrition include Tenure, MonthlyCharges, InternetService, PaymentMethod, Contract, OnlineSecurity, TechSupport, and PaperlessBilling. We also found that the most significant relationships from our logistic model are the customerâ€™s monthly charges, the type of internet service and contract they have, and the length of time they have been customers with Telco. To proactively reduce their churn rate, Telco could target customers who are on month-to-month contracts, use fiber optic internet, have higher monthly charges on average, and who have a shorter tenure of less than 18 months, which is the average tenure of their former customers.

##6.1 | References
Chawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer, W. P. (2002). Smote: Synthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321-357.

Hanley, J. A., & Mcneil, B. J. (1982). The Meaning and Use of the Area Under a Receiver Operating Characteristic (ROC) Curve. Radiology, 143(1), 29-36. doi:10.1148/radiology.143.1.7063747

Hastie, T., Tibshirani, R., & Friedman, J. H. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd ed. New York, NY: Springer.

James, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An Introduction to Statistical Learning with Applications in R. New York, NY: Springer.

Torgo, L. (2010) Data Mining using R: learning with case studies, CRC Press (ISBN: 9781439810187). http://www.dcc.fc.up.pt/~ltorgo/DataMiningWithR


**CHAPTER IV: BUSINESS INTELLIGENCE INTERACTIVE DASHBOARDS**

## **4.1 Overview of the Dashboard**
The interactive dashboard provides insights into telecom customer churn, displaying key metrics such as churn rate, revenue, average monthly charges, and customer tenure. This analysis helps in understanding the factors influencing customer churn and retention.

## **4.2 Key Metrics Summary**
- **Total Customers:** 7032
- **Churned Customers:** 1869
- **Churn Rate:** 26.58%
- **Total Revenue:** $16.06M
- **Average Monthly Charges:** $64.80
- **Average Tenure:** 32 months

These summary metrics offer a high-level perspective on the customer base and churn patterns.

## **4.3 Churn Analysis by Billing & Payment Methods**
ðŸ“Š **Charts Used: Bar Charts**
- Customers with **paperless billing** exhibit a higher churn rate compared to those without it.
- **Electronic check payments** have the highest churn rate, while **bank transfers and credit cards** have lower churn rates.

ðŸ” **Insight:**
- Digital payment users (especially those using electronic checks) may face usability or cost-related concerns, requiring further analysis.

## **4.4 Churn by Tenure Segments**
ðŸ“Š **Charts Used: Bar Chart & Average Tenure Comparison**
- **New customers (0-18 months) churn more frequently.**
- **Longer tenure customers have significantly lower churn.**
- Average tenure of churned customers is **18 months**, whereas retained customers have an average tenure of **38 months**.

ðŸ” **Insight:**
- Targeting newer customers with better onboarding and engagement strategies can reduce churn.

## **4.5 Churn by Customer Demographics**
ðŸ“Š **Charts Used: Bar Charts**
- Customers without partners churn at a higher rate.
- Customers without dependents are more likely to leave the service.
- **Senior citizens have a higher churn rate compared to younger customers.**

ðŸ” **Insight:**
- Customers with family responsibilities (partners and dependents) are more likely to stay, possibly due to the necessity of reliable telecom services.

## **4.6 Contract Type and Churn**
ðŸ“Š **Charts Used: Pie Chart**
- **Month-to-month contract holders have the highest churn rate.**
- **Customers on one-year and two-year contracts have significantly lower churn.**

ðŸ” **Insight:**
- Encouraging long-term contracts can reduce churn through incentives like discounts or bundled services.

## **4.7 Impact of Number of Services on Churn**
ðŸ“Š **Charts Used: Line Chart & Stacked Bar Chart**
- Customers subscribing to **fewer services churn more.**
- The highest churn rate is seen among customers with **only 1 or 2 services**.
- Customers with **9 services have the lowest churn rate (5.3%)**.

ðŸ” **Insight:**
- **Upselling additional services** (e.g., streaming, online security) can enhance customer retention.

## **4.8 Churn by Service Type**
ðŸ“Š **Charts Used: Bar Charts**
- **Fiber optic users churn more than DSL users**.
- Customers with **phone and internet services together** show a lower churn rate.

ðŸ” **Insight:**
- Service quality issues with fiber optic plans could be leading to higher churn rates.

## **4.9 Subscription Trends**
ðŸ“Š **Charts Used: Bar Chart**
- **Phone service and internet service have the highest subscription rates**.
- **Security services, backup services, and device protection have lower adoption rates.**

ðŸ” **Insight:**
- Promoting add-on services can help increase customer engagement and reduce churn.

## **4.10 Recommendations**
âœ… Offer **discounted long-term contracts** to reduce churn.
âœ… Improve customer **onboarding and engagement** strategies for new users.
âœ… Analyze and **improve service quality** for fiber optic users.
âœ… Promote **bundled service packages** to retain customers.

---

This chapter provides a structured analysis of the dashboard insights, helping to drive strategic decisions for churn reduction and revenue growth.


AverageTenure = AVERAGE('Telco-Customer-Churn'[tenure])

AvgMonthlyCharges = DIVIDE(
    SUM('Telco-Customer-Churn'[MonthlyCharges]),
    COUNTROWS('Telco-Customer-Churn'))

ChurnedCustomers = CALCULATE(
    COUNTROWS('Telco-Customer-Churn'), 
    'Telco-Customer-Churn'[Churn] = "Yes"
)

ChurnRate = DIVIDE('Telco-Customer-Churn'[ChurnedCustomers],COUNT('Telco-Customer-Churn'[customerID]))

NumberOfServices = 
    IF([PhoneService] = "Yes", 1, 0) +
    IF([MultipleLines] = "Yes", 1, 0) +
    IF([InternetService] = "DSL"|| [InternetService] = "Fiber Optic", 1, 0) +
    IF([OnlineSecurity] = "Yes", 1, 0) +
    IF([OnlineBackup] = "Yes", 1, 0) +
    IF([DeviceProtection] = "Yes", 1, 0) +
    IF([TechSupport] = "Yes", 1, 0) +
    IF([StreamingTV] = "Yes", 1, 0) +
    IF([StreamingMovies] = "Yes", 1, 0)

RetentionRate = 
CALCULATE(
    COUNTROWS('Telco-Customer-Churn'), 
    'Telco-Customer-Churn'[Churn] = "No"
) / COUNTROWS('Telco-Customer-Churn')

RevenueLost = CALCULATE(
    SUM('Telco-Customer-Churn'[TotalCharges]), 
    'Telco-Customer-Churn'[Churn] = "Yes"
)

ServiceCombination = 
    SWITCH(
        TRUE(),
        'Telco-Customer-Churn'[PhoneService] = "Yes" && 'Telco-Customer-Churn'[InternetService] IN {"Fiber optic", "DSL"}, "Phone & Internet Users",
        'Telco-Customer-Churn'[PhoneService] = "Yes" && 'Telco-Customer-Churn'[InternetService] = "No", "Only Phone Users",
        'Telco-Customer-Churn'[PhoneService] = "No" && 'Telco-Customer-Churn'[InternetService] IN {"Fiber optic", "DSL"}, "Only Internet Users",
        "Unknown"
    )

TenureSegment = 
    SWITCH(
        TRUE(),
        [Tenure] >= 1 && [Tenure] <= 12, "New Customer",
        [Tenure] >= 13 && [Tenure] <= 24, "Short-term Customer",
        [Tenure] >= 25 && [Tenure] <= 48, "Medium-term Customer",
        [Tenure] >= 49 && [Tenure] <= 72, "Long-term Customer",
        "Unknown"
    )

TotalCustomers = CALCULATE(COUNTROWS('Telco-Customer-Churn'))

TotalRevenue = SUM('Telco-Customer-Churn'[TotalCharges])

Subscribed % = DIVIDE('Telco-Customer-Churn (2)'[Count], [TotalCustomers1])

TotalCustomers1 = CALCULATE(COUNTROWS('Telco-Customer-Churn'))





